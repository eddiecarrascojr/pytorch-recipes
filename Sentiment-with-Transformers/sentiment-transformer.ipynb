{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9318fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d5e5e4",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Transformers provide an accurate and fast way to test NLP problem where context is needed. Here we will use a pre-trained model to re-train for the IMDB movie review datasets. \n",
    "Libraries used:\n",
    "- PyTorch\n",
    "- Transformers\n",
    "- Numpy\n",
    "- Scikit-learn\n",
    "\n",
    "\n",
    "Sample architecture of a ![Transformers](\"assets\\images\\Transformer_full_architecture.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47c0321",
   "metadata": {},
   "source": [
    "## Load the IMBD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c94c586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5901efb2ba9349abbeb92752e894a1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eduar\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\eduar\\.cache\\huggingface\\hub\\datasets--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b322b552b577480a893b7886f5d7d45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3818ef2efa147c2965a831873b06bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8eaf770a0842b190fe55956cf99997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61517315fa974fb69bbf251cde874d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36cb409e782a4388a72c8481baa393ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730c0a6b10174106a2441cfac9fdb5f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1000 training examples and 1000 testing examples.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading IMDB dataset...\")\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "# For faster development and testing, you can use a smaller subset of the data.\n",
    "# We'll create a smaller training and test set by shuffling and selecting a subset.\n",
    "small_train_dataset = imdb[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_test_dataset = imdb[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "print(f\"Using {len(small_train_dataset)} training examples and {len(small_test_dataset)} testing examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece97e66",
   "metadata": {},
   "source": [
    "## Load a pre-trained tokenizer\n",
    "Every Transformer model has a corresponding tokenizer that converts text into a format the model can understand (input IDs, attention mask, etc.).\n",
    "We'll use the tokenizer for 'distilbert-base-uncased', a smaller and faster version of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da55446d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60a1d7534ab4ae4855c6c6ee62497f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eduar\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\eduar\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3788b8f104f4d54880ca4bef8a688d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633548b1a98744fb8795df93e0cb821c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd38604a6d1e46c18e45a4e294501bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4e46e3",
   "metadata": {},
   "source": [
    "## Tokenize the dataset\n",
    "We'll create a function to tokenize the text in our dataset.  \n",
    "\n",
    "`truncation=True` ensures that long reviews are cut to the model's max length.\n",
    "\n",
    "`padding=True` adds padding to shorter reviews to make all inputs the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc6f1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fafe2348375495f95a6a08522eceda0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56efb785d8ec48ceb7bb36d0298bee3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train_dataset = small_train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = small_test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d06c77",
   "metadata": {},
   "source": [
    "## Load a pre-trained model\n",
    "\n",
    "We'll load 'distilbert-base-uncased' with a sequence classification head.\n",
    "\n",
    "`num_labels=2` specifies that this is a binary classification problem (positive/negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94017d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c4e4016f0d460cb33e21f4e666ea4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading pre-trained model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0581fb8d",
   "metadata": {},
   "source": [
    "## Define training arguments\n",
    "\n",
    "`TrainingArguments` is a class that contains all the hyperparameters for training.\n",
    "\n",
    "This includes settings like learning rate, number of epochs, batch size, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18286ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eduar\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # Directory to save the model and results\n",
    "    num_train_epochs=3,              # Total number of training epochs\n",
    "    per_device_train_batch_size=16,  # Batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1e948f",
   "metadata": {},
   "source": [
    "## Define evaluation metrics\n",
    "\n",
    "We need a function to compute metrics during evaluation.\n",
    "\n",
    "This function will be called by the Trainer at each evaluation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5f61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94803156",
   "metadata": {},
   "source": [
    "## Create a Trainer instance\n",
    "\n",
    "The `Trainer` class provides a high-level API for training and evaluating\n",
    "\n",
    "Hugging Face Transformers models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c78bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c74521",
   "metadata": {},
   "source": [
    "#  Train the model\n",
    "Use transfer learning to train the model.\n",
    "\n",
    "Calling `train()` on the Trainer instance will start the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0570ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 00:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.691700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.702600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.698300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.695700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.695200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.689500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.682100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.674100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.668700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.629000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.564600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.518700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.402300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.404800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.314700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.316300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.205800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.317100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "0.6917\n",
      "Attempted to log scalar metric grad_norm:\n",
      "0.9115180969238281\n",
      "Attempted to log scalar metric learning_rate:\n",
      "9e-07\n",
      "Attempted to log scalar metric epoch:\n",
      "0.15873015873015872\n",
      "Attempted to log scalar metric loss:\n",
      "0.7026\n",
      "Attempted to log scalar metric grad_norm:\n",
      "0.9072819352149963\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.9e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "0.31746031746031744\n",
      "Attempted to log scalar metric loss:\n",
      "0.6983\n",
      "Attempted to log scalar metric grad_norm:\n",
      "1.3646469116210938\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.9e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "0.47619047619047616\n",
      "Attempted to log scalar metric loss:\n",
      "0.6957\n",
      "Attempted to log scalar metric grad_norm:\n",
      "0.9887279272079468\n",
      "Attempted to log scalar metric learning_rate:\n",
      "3.9e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "0.6349206349206349\n",
      "Attempted to log scalar metric loss:\n",
      "0.6952\n",
      "Attempted to log scalar metric grad_norm:\n",
      "1.1940356492996216\n",
      "Attempted to log scalar metric learning_rate:\n",
      "4.9000000000000005e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "0.7936507936507936\n",
      "Attempted to log scalar metric loss:\n",
      "0.6895\n",
      "Attempted to log scalar metric grad_norm:\n",
      "0.9501526951789856\n",
      "Attempted to log scalar metric learning_rate:\n",
      "5.9e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "0.9523809523809523\n",
      "Attempted to log scalar metric loss:\n",
      "0.6821\n",
      "Attempted to log scalar metric grad_norm:\n",
      "2.0313549041748047\n",
      "Attempted to log scalar metric learning_rate:\n",
      "6.900000000000001e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "1.1111111111111112\n",
      "Attempted to log scalar metric loss:\n",
      "0.6741\n",
      "Attempted to log scalar metric grad_norm:\n",
      "1.2301071882247925\n",
      "Attempted to log scalar metric learning_rate:\n",
      "7.9e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "1.2698412698412698\n",
      "Attempted to log scalar metric loss:\n",
      "0.6687\n",
      "Attempted to log scalar metric grad_norm:\n",
      "3.2053229808807373\n",
      "Attempted to log scalar metric learning_rate:\n",
      "8.9e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "1.4285714285714286\n",
      "Attempted to log scalar metric loss:\n",
      "0.629\n",
      "Attempted to log scalar metric grad_norm:\n",
      "2.229207754135132\n",
      "Attempted to log scalar metric learning_rate:\n",
      "9.900000000000002e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "1.5873015873015874\n",
      "Attempted to log scalar metric loss:\n",
      "0.5646\n",
      "Attempted to log scalar metric grad_norm:\n",
      "2.3992886543273926\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.09e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.746031746031746\n",
      "Attempted to log scalar metric loss:\n",
      "0.5187\n",
      "Attempted to log scalar metric grad_norm:\n",
      "3.9442291259765625\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.19e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.9047619047619047\n",
      "Attempted to log scalar metric loss:\n",
      "0.4023\n",
      "Attempted to log scalar metric grad_norm:\n",
      "3.678611993789673\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.29e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0634920634920633\n",
      "Attempted to log scalar metric loss:\n",
      "0.4048\n",
      "Attempted to log scalar metric grad_norm:\n",
      "9.212882041931152\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.3900000000000002e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "2.2222222222222223\n",
      "Attempted to log scalar metric loss:\n",
      "0.3147\n",
      "Attempted to log scalar metric grad_norm:\n",
      "3.5744404792785645\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.49e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "2.380952380952381\n",
      "Attempted to log scalar metric loss:\n",
      "0.3163\n",
      "Attempted to log scalar metric grad_norm:\n",
      "10.86209774017334\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.59e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "2.5396825396825395\n",
      "Attempted to log scalar metric loss:\n",
      "0.2058\n",
      "Attempted to log scalar metric grad_norm:\n",
      "14.224045753479004\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.69e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "2.6984126984126986\n",
      "Attempted to log scalar metric loss:\n",
      "0.3171\n",
      "Attempted to log scalar metric grad_norm:\n",
      "11.616868019104004\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.79e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "2.857142857142857\n",
      "Attempted to log scalar metric train_runtime:\n",
      "38.9695\n",
      "Attempted to log scalar metric train_samples_per_second:\n",
      "76.983\n",
      "Attempted to log scalar metric train_steps_per_second:\n",
      "4.85\n",
      "Attempted to log scalar metric total_flos:\n",
      "397402195968000.0\n",
      "Attempted to log scalar metric train_loss:\n",
      "0.5349379910363091\n",
      "Attempted to log scalar metric epoch:\n",
      "3.0\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting model training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eebaf18",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "After training, you can evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a254539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric eval_loss:\n",
      "0.3231147527694702\n",
      "Attempted to log scalar metric eval_accuracy:\n",
      "0.858\n",
      "Attempted to log scalar metric eval_f1:\n",
      "0.8453159041394336\n",
      "Attempted to log scalar metric eval_precision:\n",
      "0.9023255813953488\n",
      "Attempted to log scalar metric eval_recall:\n",
      "0.7950819672131147\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "4.4084\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "226.841\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "3.629\n",
      "Attempted to log scalar metric epoch:\n",
      "3.0\n",
      "Evaluation results: {'eval_loss': 0.3231147527694702, 'eval_accuracy': 0.858, 'eval_f1': 0.8453159041394336, 'eval_precision': 0.9023255813953488, 'eval_recall': 0.7950819672131147, 'eval_runtime': 4.4084, 'eval_samples_per_second': 226.841, 'eval_steps_per_second': 3.629, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating the model on the test set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1033681b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall results:  85.8\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall results: \", 100*eval_results[\"eval_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4020988e",
   "metadata": {},
   "source": [
    "## Make predictions on new text\n",
    "You can now use your fine-tuned model to predict the sentiment of new sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad9afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Move tensors to the same device as the model\n",
    "    device = model.device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Get the predicted class (0 for negative, 1 for positive)\n",
    "    predicted_class_id = torch.argmax(logits, dim=1).item()\n",
    "    return \"Positive\" if predicted_class_id == 1 else \"Negative\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063c4605",
   "metadata": {},
   "source": [
    "# Evaluate our model\n",
    "Let's see how well the transfer learning did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50e3f166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: 'This movie was fantastic! I really enjoyed the acting and the plot.'\n",
      "Predicted sentiment: Positive\n",
      "Review: 'It was a complete waste of time. The story was boring and predictable.'\n",
      "Predicted sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "# Example predictions\n",
    "review1 = \"This movie was fantastic! I really enjoyed the acting and the plot.\"\n",
    "review2 = \"It was a complete waste of time. The story was boring and predictable.\"\n",
    "\n",
    "print(f\"Review: '{review1}'\")\n",
    "print(f\"Predicted sentiment: {predict_sentiment(review1)}\")\n",
    "\n",
    "print(f\"Review: '{review2}'\")\n",
    "print(f\"Predicted sentiment: {predict_sentiment(review2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b39922",
   "metadata": {},
   "source": [
    "## Results\n",
    "It appears using a Transformer yield good results with very little specific model selection and re-training.\n",
    "This is a great approach with starting from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2cdcf2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
